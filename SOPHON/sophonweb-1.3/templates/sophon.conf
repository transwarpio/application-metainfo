####################
# Livy Config
####################

# Use this keystore for the SSL certificate and key.
# livy.keystore =

# Specify the keystore password.
# livy.keystore.password =

# What host address to start the server on. By default, Livy will bind to all network interfaces.
# livy.server.host = 0.0.0.0

# What port to start the server on.
# livy.server.port = 6066

# What spark master Livy sessions should use.
livy.spark.master = yarn-client
spark.livy.spark_major_version = 2
# What spark deploy mode Livy sessions should use.
# livy.spark.deployMode =

# Time in milliseconds on how long Livy will wait before timing out an idle session.
# livy.server.session.timeout = 1h

# If livy should impersonate the requesting users when creating a new session.
# livy.impersonation.enabled = true

# Comma-separated list of Livy RSC jars. By default Livy will upload jars from its installation
# directory every time a session is started. By caching these files in HDFS, for example, startup
# time of sessions on YARN can be reduced.
# livy.jars =

# Comma-separated list of Livy REPL jars. By default Livy will upload jars from its installation
# directory every time a session is started. By caching these files in HDFS, for example, startup
# time of sessions on YARN can be reduced.
# livy.repl.jars =

# Location of PySpark archives. By default Livy will upload the file from SPARK_HOME, but
# by caching the file in HDFS, startup time of PySpark sessions on YARN can be reduced.
# livy.pyspark.archives =

# Location of the SparkR package. By default Livy will upload the file from SPARK_HOME, but
# by caching the file in HDFS, startup time of R sessions on YARN can be reduced.
# livy.sparkr.package =

# List of local directories from where files are allowed to be added to user sessions. By
# default it's empty, meaning users can only reference remote URIs when starting their
# sessions.
# livy.file.local-dir-whitelist =

# Whether to enable csrf protection, by default it is false. If it is enabled, client should add
# http-header "X-Requested-By" in request if the http method is POST/DELETE/PUT/PATCH.
# livy.server.csrf_protection.enabled =

# Whether to enable HiveContext in livy interpreter, if it is true hive-site.xml will be detected
# on user request and then livy server classpath automatically.
# livy.repl.enableHiveContext =

# Recovery mode of Livy. Possible values:
# off: Default. Turn off recovery. Every time Livy shuts down, it stops and forgets all sessions.
# recovery: Livy persists session info to the state store. When Livy restarts, it recovers
#           previous sessions from the state store.
# Must set livy.server.recovery.state-store and livy.server.recovery.state-store.url to
# configure the state store.
# livy.server.recovery.mode = off

# Where Livy should store state to for recovery. Possible values:
# <empty>: Default. State store disabled.
# filesystem: Store state on a file system.
# zookeeper: Store state in a Zookeeper instance.
# livy.server.recovery.state-store =

# For filesystem state store, the path of the state store directory. Please don't use a filesystem
# that doesn't support atomic rename (e.g. S3). e.g. file:///tmp/livy or hdfs:///.
# For zookeeper, the address to the Zookeeper servers. e.g. host1:port1,host2:port2
# livy.server.recovery.state-store.url =

# If Livy can't find the yarn app within this time, consider it lost.
livy.server.yarn.app-lookup-timeout = 360s

# How often Livy polls YARN to refresh YARN app state.
# livy.server.yarn.poll-interval = 1s

# WARMING!!! following are for developing and debug use, do not use them in production environment
# do not create new process for session, make debug easier
livy.rsc.client.do-not-use.run-driver-in-process=false

# develop mode, will do extra validation
# livy.server.dev-mode=false

# use manager system
livy.server.user.manager.open = ${service['livy.server.user.manager.open']}
livy.server.user.manager.component = midas1

# redis server for cache
livy.cache.type=${service['livy.cache.type']}
livy.cache.host=${service.roles.SOPHON_REDIS[0].hostname}
livy.cache.port=${service['redis.port']}
livy.cache.password=${service['redis.password']}


# hbase
sophon.hbase.namespace = sophon

# spark scheduler
livy.use.fair.scheduler = false


##############################
# Sophon's own config
##############################
# secret
sophon.auth.secret = simpleSecret

# storage dir
sophon.basedir = /home/root/sophon

# hdfs storage base dir
sophon.hdfs-asset-dir = /tmp

sophon.cache.host = ${service.roles.SOPHON_REDIS[0].hostname}
sophon.cache.port = ${service['redis.port']}
sophon.cache.password = ${service['redis.password']}

# web socket port
sophon.socket.port = 9099

# api service launch mode: thread, process or docker
sophon.apiservice.launch-mode = thread

# license
<#assign license_servers=[]>
<#list dependencies.LICENSE_SERVICE.roles.LICENSE_NODE as server>
    <#assign license_servers += [(server.hostname + ":" + dependencies.LICENSE_SERVICE[server.hostname]["zookeeper.client.port"])]>
</#list>
sophon.license.url = ${license_servers?join(",")}
sophon.license.type = customized

# kong api gateway
<#if dependencies.KONG??>
sophon.apiservice.launch-mode = thread
sophon.kong.admin-url = http://${dependencies.KONG.roles.KONG_SERVER[0]['hostname']}:${dependencies.KONG['kong.admin.port']}
sophon.kong.proxy-url = http://${dependencies.KONG.roles.KONG_SERVER[0]['hostname']}:${dependencies.KONG['kong.proxy.port']}
sophon.kong.auth = basic
sophon.kong.username = admin
sophon.kong.password = passwd
</#if>
# job history
# history timeout in days, job history will be auto removed after reaching timeout.
# set it to negative value like -1 if you want to keep history forever.
sophon.job.history.timeout = -1
<#if dependencies.WORKFLOW??>
<#assign workflowHostPorts = []/>
<#list dependencies.WORKFLOW.roles['WORKFLOW_SERVER'] as role>
<#assign workflowHostPorts = workflowHostPorts + [role.hostname + ':' + dependencies.WORKFLOW['workflow.http.port']]>
</#list>
<#assign workflowHostPort = workflowHostPorts?join(",")>
sophon.workflow.url=${workflowHostPort}
</#if>
sophon.visit.port = ${service['sophon.visit.port']}
sophon.proxy.api.port = ${service['sophon.proxy.api.port']}
# guardian mode
<#if service['sophon.enable.kerberos'] == 'true'>
sophon.keytab = ${service.keytab}
sophon.principal = hive/${localhostname?lower_case}@${service.realm}
</#if>

# spark conf
spark.driver.memory      =   18g
spark.executor.instances =   3
spark.driver.cores       =   4
spark.executor.memory    =   18g
spark.executor.cores     =   4


