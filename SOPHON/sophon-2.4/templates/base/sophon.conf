####################
# Livy Config
####################

# Use this keystore for the SSL certificate and key.
# livy.keystore =

# Specify the keystore password.
# livy.keystore.password =

# What host address to start the server on. By default, Livy will bind to all network interfaces.
# livy.server.host = 0.0.0.0

# What port to start the server on.
# livy.server.port = 6066

spark.scheduler.allocation.file=/etc/${service.sid}/conf/scheduler.xml
spark.scheduler.mode = FAIR

# What spark master Livy sessions should use.
livy.server.session.timeout-check=true
livy.rsc.launcher.port.range=10000~10100

# k8s auth
sophon.k8s.ca.cert.file = /srv/kubernetes/ca.crt
sophon.k8s.client.cert.file = /srv/kubernetes/admin.pem
sophon.k8s.client.key.file = /srv/kubernetes/admin-key.pem

<#if service['sophon.resource.type'] = "yarn">
livy.spark.master = yarn-client
livy.server.yarn.app-lookup-timeout = 360s
<#else>
##############################
##  K8s config
###############################
livy.spark.master = k8s://https://${dependencies.TOS.roles.TOS_MASTER[0]['hostname']}:6443
livy.rsc.server.connect.timeout = 300s

spark.kubernetes.driver.annotation.cni.networks = overlay
spark.kubernetes.executor.annotation.cni.networks = overlay
<#if dependencies.HDFS??>
spark.kubernetes.driver.volumes.hostPath.hdfs.mount.path=/etc/${dependencies.HDFS.sid}/conf
spark.kubernetes.driver.volumes.hostPath.hdfs.options.path=/etc/${dependencies.HDFS.sid}/conf
spark.kubernetes.executor.volumes.hostPath.hdfs.mount.path=/etc/${dependencies.HDFS.sid}/conf
spark.kubernetes.executor.volumes.hostPath.hdfs.options.path=/etc/${dependencies.HDFS.sid}/conf
</#if>
<#if dependencies.HYPERBASE??>
spark.kubernetes.driver.volumes.hostPath.hyperbase.mount.path=/etc/${dependencies.HYPERBASE.sid}/conf
spark.kubernetes.driver.volumes.hostPath.hyperbase.options.path=/etc/${dependencies.HYPERBASE.sid}/conf
spark.kubernetes.executor.volumes.hostPath.hyperbase.mount.path=/etc/${dependencies.HYPERBASE.sid}/conf
spark.kubernetes.executor.volumes.hostPath.hyperbase.options.path=/etc/${dependencies.HYPERBASE.sid}/conf
spark.driver.extraClassPath = /etc/${dependencies.HDFS.sid}/conf:/etc/${dependencies.HYPERBASE.sid}/conf/
spark.executor.extraClassPath = /etc/${dependencies.HDFS.sid}/conf:/etc/${dependencies.HYPERBASE.sid}/conf:/usr/lib/sophon/hadoop-jars/*
</#if>

# spark.kubernetes.driver.volumes.hostPath.sophonconf.mount.path=/etc/${service.sid}/conf
# spark.kubernetes.driver.volumes.hostPath.sophonconf.options.path=/etc/${service.sid}/conf
spark.kubernetes.driver.volumes.hostPath.transwarphosts.mount.path=/etc/transwarp/conf
spark.kubernetes.driver.volumes.hostPath.transwarphosts.options.path=/etc/transwarp/conf
spark.kubernetes.driver.volumes.hostPath.yarn.mount.path=/etc/yarn1/conf
spark.kubernetes.driver.volumes.hostPath.yarn.options.path=/etc/yarn1/conf

spark.kubernetes.authenticate.submission.caCertFile = /srv/kubernetes/ca.crt
spark.kubernetes.authenticate.submission.clientKeyFile = /srv/kubernetes/admin-key.pem
spark.kubernetes.authenticate.submission.clientCertFile = /srv/kubernetes/admin.pem
spark.kubernetes.authenticate.driver.caCertFile = /srv/kubernetes/ca.crt
spark.kubernetes.authenticate.driver.clientKeyFile = /srv/kubernetes/admin-key.pem
spark.kubernetes.authenticate.driver.clientCertFile = /srv/kubernetes/admin.pem
spark.kubernetes.driver.container.image = ${service['driver.docker.image.name']}
spark.kubernetes.executor.container.image = ${service['driver.docker.image.name']}
spark.kubernetes.executor.volumes.hostPath.yarn.mount.path=/etc/yarn1/conf
spark.kubernetes.executor.volumes.hostPath.yarn.options.path=/etc/yarn1/conf
spark.kubernetes.container.image.pullPolicy=Always
spark.kubernetes.driverEnv.SOPHON_CONF_DIR=/etc/${service.sid}/conf
spark.kubernetes.namespace = default

spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs.options.claimName=nfs-test
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs.mount.path=/sophon/project
sophon.k8s.driver.custom.secrets.${service.sid}=/etc/${service.sid}/conf
</#if>


spark.livy.spark_major_version = 2
# What spark deploy mode Livy sessions should use.
# livy.spark.deployMode = client

# Time in milliseconds on how long Livy will wait before timing out an idle session.
# livy.server.session.timeout = 1h

# If livy should impersonate the requesting users when creating a new session.
# livy.impersonation.enabled = true

# Comma-separated list of Livy RSC jars. By default Livy will upload jars from its installation
# directory every time a session is started. By caching these files in HDFS, for example, startup
# time of sessions on YARN can be reduced.
# livy.jars =

# Comma-separated list of Livy REPL jars. By default Livy will upload jars from its installation
# directory every time a session is started. By caching these files in HDFS, for example, startup
# time of sessions on YARN can be reduced.
# livy.repl.jars =

# Location of PySpark archives. By default Livy will upload the file from SPARK_HOME, but
# by caching the file in HDFS, startup time of PySpark sessions on YARN can be reduced.
# livy.pyspark.archives =

# Location of the SparkR package. By default Livy will upload the file from SPARK_HOME, but
# by caching the file in HDFS, startup time of R sessions on YARN can be reduced.
# livy.sparkr.package =

# List of local directories from where files are allowed to be added to user sessions. By
# default it's empty, meaning users can only reference remote URIs when starting their
# sessions.
# livy.file.local-dir-whitelist =

# Whether to enable csrf protection, by default it is false. If it is enabled, client should add
# http-header "X-Requested-By" in request if the http method is POST/DELETE/PUT/PATCH.
# livy.server.csrf_protection.enabled =

# Whether to enable HiveContext in livy interpreter, if it is true hive-site.xml will be detected
# on user request and then livy server classpath automatically.
# livy.repl.enableHiveContext =

# Recovery mode of Livy. Possible values:
# off: Default. Turn off recovery. Every time Livy shuts down, it stops and forgets all sessions.
# recovery: Livy persists session info to the state store. When Livy restarts, it recovers
#           previous sessions from the state store.
# Must set livy.server.recovery.state-store and livy.server.recovery.state-store.url to
# configure the state store.
# livy.server.recovery.mode = off

# Where Livy should store state to for recovery. Possible values:
# <empty>: Default. State store disabled.
# filesystem: Store state on a file system.
# zookeeper: Store state in a Zookeeper instance.
# livy.server.recovery.state-store =

# For filesystem state store, the path of the state store directory. Please don't use a filesystem
# that doesn't support atomic rename (e.g. S3). e.g. file:///tmp/livy or hdfs:///.
# For zookeeper, the address to the Zookeeper servers. e.g. host1:port1,host2:port2
# livy.server.recovery.state-store.url =

# If Livy can't find the yarn app within this time, consider it lost.

# How often Livy polls YARN to refresh YARN app state.
# livy.server.yarn.poll-interval = 1s

# WARMING!!! following are for developing and debug use, do not use them in production environment
# do not create new process for session, make debug easier
livy.rsc.client.do-not-use.run-driver-in-process=false

# develop mode, will do extra validation
# livy.server.dev-mode=false

# use manager system
livy.server.user.manager.open = ${service['livy.server.user.manager.open']}
livy.server.user.manager.component = midas1

# redis server for cache
livy.cache.type=${service['livy.cache.type']}
livy.cache.host=${service.roles.SOPHON_REDIS[0].hostname}
livy.cache.port=${service['redis.port']}
livy.cache.password=${service['redis.password']}


# hbase
sophon.hbase.namespace = sophon

# spark scheduler
livy.use.fair.scheduler = false


##############################
# Sophon's own config
##############################
# secret
sophon.auth.secret = simpleSecret

# storage dir
sophon.basedir = /home/root/sophon

# hdfs storage base dir
sophon.hdfs-asset-dir = /tmp

sophon.cache.host = ${service.roles.SOPHON_REDIS[0].hostname}
sophon.cache.port = ${service['redis.port']}
sophon.cache.password = ${service['redis.password']}

# web socket port
sophon.socket.port = 9099

# license
<#assign license_servers=[]>
<#list dependencies.LICENSE_SERVICE.roles.LICENSE_NODE as server>
    <#assign license_servers += [(server.hostname + ":" + dependencies.LICENSE_SERVICE[server.hostname]["zookeeper.client.port"])]>
</#list>
sophon.license.url = ${license_servers?join(",")}
sophon.license.type = customized

# kong api gateway
<#if dependencies.KONG??>
sophon.kong.admin-url = http://${dependencies.KONG.roles.KONG_SERVER[0]['hostname']}:${dependencies.KONG['kong.admin.port']}
sophon.kong.proxy-url = http://${dependencies.KONG.roles.KONG_SERVER[0]['hostname']}:${dependencies.KONG['kong.proxy.port']}
sophon.kong.auth = basic
sophon.kong.username = admin
sophon.kong.password = passwd
# sophon.apiservice.replicas.limit = 5 
# sophon.apiservice.image.name = ${service['sophon.apiservice.image.name']}
</#if>
sophon.apiservice.image.name = ${service['sophon.apiservice.image.name']}
sophon.apiservice.replicas.limit = 5 

# job history
# history timeout in days, job history will be auto removed after reaching timeout.
# set it to negative value like -1 if you want to keep history forever.
sophon.job.history.timeout = -1
<#if dependencies.WORKFLOW??>
<#assign workflowHostPorts = []/>
<#list dependencies.WORKFLOW.roles['WORKFLOW_SERVER'] as role>
<#assign workflowHostPorts = workflowHostPorts + [role['ip'] + ':' + dependencies.WORKFLOW['workflow.http.port']]>
</#list>
<#assign workflowHostPort = workflowHostPorts?join(",")>
sophon.workflow.url=${workflowHostPort}
</#if>
sophon.visit.port = ${service['sophon.visit.port']}
sophon.proxy.api.port = ${service['sophon.proxy.api.port']}
# guardian mode
<#if dependencies.YARN.auth == 'kerberos'>
sophon.enable.kerberos = true
sophon.keytab = ${service.keytab}
sophon.principal = hive/${localhostname?lower_case}@${service.realm}
</#if>

# CAS
sophon.auth.cas.service.prefix=https://${dependencies.GUARDIAN.roles.CAS_SERVER[0]['hostname']}:${dependencies.GUARDIAN['cas.server.ssl.port']}/cas
# sophon.auth.app.service.prefix=https://${service.roles.SOPHON_WEB_UI[0].hostname}:${service['sophon.ui.port']}
sophon.auth.disable.ssl.check=true
sophon.auth.disable.cas.proxy=true
sophon.auth.cas.session.timeout = ${service['sophon.auth.cas.session.timeout']}
# stream
sophon.streaming.result.cache.size=5
sophon.streaming.result.size.in.redis=100
sophon.streaming.result.sample.percent=0.1

# spark conf
spark.driver.memory      =   18g
spark.executor.instances =   3
spark.driver.cores       =   4
spark.executor.memory    =   18g
spark.executor.cores     =   4

#notebook 
sophon.k8s.nfs.dir=/sophon/project
sophon.k8s.nfs.pvc.name=nfs-test
sophon.k8s.tensorboard.image.name = ${service['sophon.k8s.tensorboard.image.name']}
sophon.k8s.running.timeout = 120000
sophon.k8s.worker.instances = 2
sophon.k8s.ps.instances = 2
sophon.k8s.jupyter.cpu.image.name = ${service['sophon.k8s.jupyter.cpu.image.name']}
sophon.k8s.jupyter.gpu.image.name = ${service['sophon.k8s.jupyter.gpu.image.name']}
sophon.k8s.api.server = https://${dependencies.TOS.roles.TOS_MASTER[0]['hostname']}:6443
sophon.k8s.namespace = default
sophon.k8s.resource = cpu:2,memory:4Gi
sophon.pypi.server.url = http://${localhostname}:${service['python.repo.port']}
sophon.rrepo.server.url = ftp://${localhostname}:${service['r.repo.port']}/rlibs
sophon.pypi.server.username = sophonuser
sophon.pypi.server.password = password
sophon.rrepo.server.username = sophonuser
sophon.rrepo.server.password = password



sophon.data.downloadable = ${service['sophon.data.downloadable']}
sophon.data.preview.limit = ${service['sophon.data.preview.limit']}

# sophon share
sophon.share-server.url = http://${service.roles.SOPHON_SHARE_SERVER[0].hostname}:8068

# kg-explorer
sophon.kg.explorer.base.url = http://${service.roles.KG_EXPLORER[0].hostname}:9000

# https
sophon.use.https = ${service['use.https']}

# custom configurations
<#if service['sophon.conf']??>
<#list service['sophon.conf'] as key, value>
${key}=${value}
</#list>
</#if>

