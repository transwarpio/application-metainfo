---
name: ARGODB_STORAGE
version: argodb-1.0
global: false
namePrefix: ArgodbStorage
friendlyName: "ArgodbStorage"
dockerImage: "transwarp/argodb-inceptor:argodb-1.0"
dependencies:
  - name: GUARDIAN
    minVersion: transwarp-6.0
    optional: true
  - name: ZOOKEEPER
    minVersion: transwarp-6.0
    optional: false
  - name: TXSQL
    minVersion: transwarp-6.0
    optional: false
  - name: INCEPTOR
    minVersion: transwarp-6.0
    optional: true
    preferred: false
  - name: HDFS
    minVersion: transwarp-6.0
    optional: true
    preferred: false
user: hive
plugins:
  - provider: GUARDIAN
    version: 0.12.0+500.51250
    useTar: false

roles:
######################
# Inceptor Metastore #
######################
- name: INCEPTOR_METASTORE
  friendlyName: "Inceptor Metastore"
  labelPrefix: "inceptor-metastore"
  category: MASTER
  frontendOperations: ["Start", "Stop", "Delete", "Scaleout"]
  deleteOpCondition:
    deletable:
      number: 2
    reject:
      number: 1
  readinessProbe:
    probe: !<exec>
      command:
      - /bin/bash
      - -c
      - netstat -an | grep ${service['hive.metastore.port']} | grep LISTEN > /dev/null
  env:
  - name: INCEPTOR_CONF_DIR
    value: /etc/${service.sid}/conf
  mountPaths: []
  resources:
    limitsMemoryKey: metastore.container.limits.memory
    limitsCpuKey: metastore.container.limits.cpu
    requestsMemoryKey: metastore.container.requests.memory
    requestsCpuKey: metastore.container.requests.cpu
  summaryPolicy: SOME
  autoAssign:
  - advice: !<NumSeq> {"numSeq": [1]}
  suggestion:
  - criteria: !<Range> {"min": 1}
  validation:
  - criteria: !<Range> {"min": 1}
  operations:
  - type: Install
    directives:
    - directive: !<mkdirs>
        paths: ["/var/run/${service.sid}"]
        mode: "755"

#########
# Shiva #
#########
- name: SHIVA_MASTER
  friendlyName: "Shiva Master"
  labelPrefix: "shiva-master"
  dockerImage: "transwarp/argodb-shiva-master:argodb-1.0"
  category: MASTER
  frontendOperations: ["Start", "Stop", "Delete", "Scaleout"]
  readinessProbe:
    probe: !<httpGet>
      path: /
      port: ${service['shiva.master.rpc_service.master_service_port']}
      scheme: HTTP
  summaryPolicy: MAJOR
  autoAssign:
  - advice: !<NumSeq> {"numSeq": [5, 3]}
  suggestion:
  - criteria: !<Range> {"min": 3, "oddity": true}
  validation:
  - criteria: !<Range> {"min": 1}
  resources:
    limitsMemoryKey: shiva.master.container.limits.memory
    limitsCpuKey: shiva.master.container.limits.cpu
    requestsMemoryKey: shiva.master.container.requests.memory
    requestsCpuKey: shiva.master.container.requests.cpu
  nodeSpecificMounts:
  - configKey: shiva.master.master.data_path
  env:
  - name: CONF_DIR
    value: /etc/${service.sid}/conf/shiva
  deleteOpCondition:
    deletable:
      number: 3
      decommission: false
    reject:
      number: 2
  mountPaths: []
  operations:
  - type: Install
    directives:
    - directive: !<mkdirs>
        paths: ["/etc/${service.sid}/conf/shiva"]
        mode: "755"
  - type: Config
    directives:
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "shiva/master.conf"
        targetPath: "/etc/${service.sid}/conf/shiva/master.conf"
        mode: "755"

- name: SHIVA_WEBSERVER
  friendlyName: "Shiva Webserver"
  labelPrefix: "shiva-webserver"
  dockerImage: "transwarp/argodb-shiva-webserver:argodb-1.0"
  category: MASTER
  frontendOperations: ["Start", "Stop", "Delete"]
  readinessProbe:
    probe: !<httpGet>
      path: /
      port: ${service['shiva.http.port']}
      scheme: HTTP
  summaryPolicy: MAJOR
  autoAssign:
  - advice: !<NumSeq> {"numSeq": [1]}
  suggestion:
  - criteria: !<Range> {"min": 1, "oddity": true}
  validation:
  - criteria: !<Range> {"min": 1}
  linkExpression: "http://${localhostname}:${service['shiva.http.port']}"
  resources:
    limitsMemoryKey: shiva.webserver.container.limits.memory
    limitsCpuKey: shiva.webserver.container.limits.cpu
    requestsMemoryKey: shiva.webserver.container.requests.memory
    requestsCpuKey: shiva.webserver.container.requests.cpu
  env:
  - name: MASTER_GROUP
    value: |
      <#function master_group>
        <#local group = []>
        <#list service.roles["SHIVA_MASTER"] as r>
          <#local group = group + [r.ip + ":" + service['shiva.master.rpc_service.master_service_port']]>
        </#list>
        <#return group?join(",")>
      </#function>
      ${master_group()}
  - name: CONF_DIR
    value: /etc/${service.sid}/conf/shiva
  deleteOpCondition:
    deletable:
      number: 2
    reject:
      number: 1
  mountPaths: []
  operations:
  - type: Install
    directives:
    - directive: !<mkdirs>
        paths: ["/etc/${service.sid}/conf/shiva"]
        mode: "755"
  - type: Config
    directives:
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "shiva/shiva-restful.sh"
        targetPath: "/etc/${service.sid}/conf/shiva/shiva-restful.sh"
        mode: "755"
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "shiva/log4j.properties"
        targetPath: "/etc/${service.sid}/conf/shiva/log4j.properties"
        mode: "755"

- name: SHIVA_TABLETSERVER
  friendlyName: "Shiva Tablet Server"
  labelPrefix: "shiva-tabletserver"
  dockerImage: "transwarp/argodb-shiva-tabletserver:argodb-1.0"
  category: SLAVE
  frontendOperations: ["Start", "Stop", "Delete", "Scaleout"]
  readinessProbe:
    probe: !<httpGet>
      path: /
      port: ${service['shiva.tabletserver.rpc_service.manage_service_port']}
      scheme: HTTP
  summaryPolicy: MAJOR
  autoAssign:
  - advice: !<EachNode> {}
  suggestion:
  - criteria: !<Range> {"min": 3, "oddity": true}
  validation:
  - criteria: !<Range> {"min": 1}
  resources:
    limitsMemoryKey: shiva.tabletserver.container.limits.memory
    limitsCpuKey: shiva.tabletserver.container.limits.cpu
    requestsMemoryKey: shiva.tabletserver.container.requests.memory
    requestsCpuKey: shiva.tabletserver.container.requests.cpu
  nodeSpecificMounts:
  - configKey: shiva.tabletserver.store.datadirs
  env:
  - name: CONF_DIR
    value: /etc/${service.sid}/conf/shiva
  - name: LOG_DIR
    value: /var/log/${service.sid}
  - name: MASTER_GROUP
    value: |
      <#function master_group>
        <#local group = []>
        <#list service.roles["SHIVA_MASTER"] as r>
          <#local group = group + [r.ip + ":" + service['shiva.master.rpc_service.master_service_port']]>
        </#list>
        <#return group?join(",")>
      </#function>
      ${master_group()}
  deleteOpCondition:
    deletable:
      number: 3
      decommission: false
    reject:
      number: 2
  mountPaths: []
  operations:
  - type: Install
    directives:
    - directive: !<mkdirs>
        paths: ["/etc/${service.sid}/conf/shiva"]
        mode: "755"
  - type: Config
    directives:
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "shiva/tabletserver.conf"
        targetPath: "/etc/${service.sid}/conf/shiva/tabletserver.conf"
        mode: "755"
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "shiva/store.conf"
        targetPath: "/etc/${service.sid}/conf/shiva/store.conf"
        mode: "755"
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "shiva/topology.conf"
        targetPath: "/etc/${service.sid}/conf/shiva/topology.conf"
        mode: "755"

##########
# Ladder #
##########
- name: LADDER_MASTER
  friendlyName: "Ladder Master"
  labelPrefix: "ladder-master"
  dockerImage: "transwarp/argodb-ladder:argodb-1.0"
  category: MASTER
  frontendOperations: ["Start", "Stop", "Delete", "Scaleout"]
  readinessProbe:
    probe: !<httpGet>
      path: /
      port: ${service['ladder.master.web.port']}
      scheme: HTTP
  summaryPolicy: SOME
  autoAssign:
  - advice: !<NumSeq> {"numSeq": [1]}
  suggestion:
  - criteria: !<Range> {"min": 1}
  validation:
  - criteria: !<Range> {"min": 1}
  resources:
    limitsMemoryKey: ladder.master.container.limits.memory
    limitsCpuKey: ladder.master.container.limits.cpu
    requestsMemoryKey: ladder.master.container.requests.memory
    requestsCpuKey: ladder.master.container.requests.cpu
  nodeSpecificMounts:
  - configKey: ladder.master.journal.data_path
  - configKey: ladder.master.localfs.data_path
  env:
  - name: LADDER_CONF_DIR
    value: /etc/${service.sid}/conf/ladder
  - name: LADDER_LOGS_DIR
    value: /var/log/${service.sid}/ladder
  deleteOpCondition:
    deletable:
      number: 3
      decommission: false
    reject:
      number: 2
  deleteOpCleanDirs:
    fromPath:
    - key: /var/ladder_format/is_formated.identifier
      featureFile: is_formated.identifier
  mountPaths:
  - mountPath: /var/ladder_format
    hostPath: /var/transwarp/data/ladder
    name: format
  operations:
  - type: Install
    directives:
    - directive: !<mkdirs>
        paths: ["/etc/${service.sid}/conf/ladder"]
        mode: "755"
    - directive: !<mkdirs>
        paths: ["/var/transwarp/data/ladder"]
        mode: "755"
  - type: Config
    directives:
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "ladder/ladder-site.properties"
        targetPath: "/etc/${service.sid}/conf/ladder/ladder-site.properties"
        mode: "755"
    - directive: !<resource>
        templateType: Raw
        templatePath: "ladder/log4j.properties"
        targetPath: "/etc/${service.sid}/conf/ladder/log4j.properties"
        mode: "755"
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "ladder/workers"
        targetPath: "/etc/${service.sid}/conf/ladder/workers"
        mode: "755"

- name: LADDER_WORKER
  friendlyName: "Ladder Worker"
  labelPrefix: "ladder-worker"
  dockerImage: "transwarp/argodb-ladder:argodb-1.0"
  category: SLAVE
  frontendOperations: ["Start", "Stop", "Delete", "Scaleout"]
  readinessProbe:
    probe: !<httpGet>
      path: /
      port: ${service['ladder.worker.web.port']}
      scheme: HTTP
  summaryPolicy: MAJOR
  autoAssign:
  - advice: !<EachNode> {}
  suggestion:
  - criteria: !<Range> {"min": 3, "oddity": true}
  validation:
  - criteria: !<Range> {"min": 1}
  resources:
    limitsMemoryKey: ladder.worker.container.limits.memory
    limitsCpuKey: ladder.worker.container.limits.cpu
    requestsMemoryKey: ladder.worker.container.requests.memory
    requestsCpuKey: ladder.worker.container.requests.cpu
  nodeSpecificMounts:
  - configKey: ladder.worker.mem.data_path
  - configKey: ladder.worker.hdd.data_path
  env:
  - name: LADDER_CONF_DIR
    value: /etc/${service.sid}/conf/ladder
  - name: LADDER_LOGS_DIR
    value: /var/log/${service.sid}/ladder
  deleteOpCondition:
    deletable:
      number: 3
      decommission: false
    reject:
      number: 2
  deleteOpCleanDirs:
    fromPath:
    - key: /var/ladder_format/is_formated_worker.identifier
      featureFile: is_formated_worker.identifier
  mountPaths:
  - mountPath: /var/ladder_format
    hostPath: /var/transwarp/data/ladder
    name: format
  operations:
  - type: Install
    directives:
    - directive: !<mkdirs>
        paths: ["/etc/${service.sid}/conf/ladder"]
        mode: "755"
    - directive: !<mkdirs>
        paths: ["/var/transwarp/data/ladder"]
        mode: "755"
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "ladder/ladder-mount.sh.ftl"
        targetPath: "/etc/${service.sid}/conf/ladder/ladder-mount.sh"
        mode: "755"
    - directive: !<shell>
        script: "bash -e /etc/${service.sid}/conf/ladder/ladder-mount.sh"
  - type: Config
    directives:
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "ladder/ladder-site.properties"
        targetPath: "/etc/${service.sid}/conf/ladder/ladder-site.properties"
        mode: "755"
    - directive: !<resource>
        templateType: Raw
        templatePath: "ladder/log4j.properties"
        targetPath: "/etc/${service.sid}/conf/ladder/log4j.properties"
        mode: "755"
    - directive: !<resource>
        templateType: FreeMarker
        templatePath: "ladder/workers"
        targetPath: "/etc/${service.sid}/conf/ladder/workers"
        mode: "755"

stages:
# TODO Remove this Bootstrap, or open write permission in ladder on root level
#  - type: Bootstrap
#    taskGroups:
#    - !<Create-Dir-in-HDFS>
#      summaryPolicy: ALL
#      timeoutMinutes: 10
#      dirs: ["/${service.sid}"]
#      user: "hive"
#      group: "hive"
#      mod: 711
  - type: Config
    taskGroups:
    - !<Create-Database>
      dbPrefix: metastore
      dbUserConfig: javax.jdo.option.ConnectionUserName
      dbPasswordConfig: javax.jdo.option.ConnectionPassword
      timeoutMinutes: 5
      summaryPolicy: SOME
    - !<Role>
      roleType: "SHIVA_MASTER"
      operation: Config
      summaryPolicy: SOME
      timeoutMinutes: 1
    - !<Role>
      roleType: "SHIVA_WEBSERVER"
      operation: Config
      summaryPolicy: SOME
      timeoutMinutes: 1
    - !<Role>
      roleType: "SHIVA_TABLETSERVER"
      operation: Config
      summaryPolicy: SOME
      timeoutMinutes: 1
    - !<Role>
      roleType: "INCEPTOR_METASTORE"
      operation: Config
      summaryPolicy: SOME
      timeoutMinutes: 1
    - !<Role>
      roleType: "LADDER_MASTER"
      operation: Config
      summaryPolicy: SOME
      timeoutMinutes: 1
    - !<Role>
      roleType: "LADDER_WORKER"
      operation: Config
      summaryPolicy: SOME
      timeoutMinutes: 1
  - type: PreUpgrade
    taskGroups:
    - !<DockerRunPreUpgrade>
        summaryPolicy: ALL
        timeoutMinutes: 10
        roleType: INCEPTOR_METASTORE
        node: Any
  - type: PreRollback
    taskGroups:
    - !<DockerRunPreRollback>
        summaryPolicy: ALL
        timeoutMinutes: 10
        roleType: INCEPTOR_METASTORE
        node: Any

product: Inceptor

firstWizardConfigs:
######################
# Inceptor Metastore #
######################
- hive.exec.scratchdir
- hive.metastore.port
- hive.metastore.warehouse.dir
- hive.server2.authentication
- hive.server2.enabled
- hive.server2.thrift.port
- inceptor.metastore.memory
- javax.jdo.option.ConnectionPassword
- javax.jdo.option.ConnectionUserName
- metastore.container.limits.cpu
- metastore.container.limits.memory
- metastore.container.requests.cpu
- metastore.container.requests.memory
- metastore.memory.ratio
#########
# Shiva #
#########
- shiva.http.port
- shiva.master.container.limits.cpu
- shiva.master.container.limits.memory
- shiva.master.container.requests.cpu
- shiva.master.container.requests.memory
- shiva.master.master.data_path
- shiva.master.memory.ratio
- shiva.master.rpc_service.master_service_port
- shiva.store.capacity
- shiva.tabletserver.container.limits.cpu
- shiva.tabletserver.container.limits.memory
- shiva.tabletserver.container.requests.cpu
- shiva.tabletserver.container.requests.memory
- shiva.tabletserver.memory.ratio
- shiva.tabletserver.rpc_service.manage_service_port
- shiva.tabletserver.store.datadirs
- shiva.webserver.container.limits.cpu
- shiva.webserver.container.limits.memory
- shiva.webserver.container.requests.cpu
- shiva.webserver.container.requests.memory
- shiva.webserver.memory.ratio
##########
# Ladder #
##########
- ladder.master.web.port
- ladder.master.container.limits.memory
- ladder.master.container.limits.cpu
- ladder.master.container.requests.memory
- ladder.master.container.requests.cpu
- ladder.master.journal.data_path
- ladder.master.localfs.data_path
- ladder.worker.web.port
- ladder.worker.container.limits.memory
- ladder.worker.container.limits.cpu
- ladder.worker.container.requests.memory
- ladder.worker.container.requests.cpu
- ladder.worker.mem.data_path
- ladder.worker.hdd.data_path
- ladder.worker.mem.size
- ladder.worker.tieredstore.level1.dirs.quota

dashboardMetrics:
- InceptorSqlActiveTasks

pages:
- summary
- roles
- configuration
- operation
- resource_allocation
- security
- plugin

principals:
- hive
- HTTP
- kafka

healthChecks:
- category: DAEMON_CHECK
  intervalSeconds: 5
  method: !<K8sPod> {}
# TODO this shows error in Manager: scala.MatchError: STORAGE (of class java.lang.String)
#- category: VITAL_SIGN_CHECK
#  intervalSeconds: 10
#  method: !<Builtin> {}

customConfigFiles:
- hive-site.xml
